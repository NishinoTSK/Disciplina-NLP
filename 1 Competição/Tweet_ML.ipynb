{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tweet_ML.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"d2eRwzbJjKci"},"source":["CASE = 0\n","BOW = 1\n","PATH = '/content/drive/My Drive/NLP/DataSet da Primeira Competição/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5pvfGtesmHxf","executionInfo":{"status":"ok","timestamp":1618440935674,"user_tz":180,"elapsed":21697,"user":{"displayName":"ARTHUR GALDINO DANGONI","photoUrl":"","userId":"07242382303046999568"}},"outputId":"932ef652-1257-4f23-bf28-c01ae659bb9a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-8x1EH3fvd-","executionInfo":{"status":"ok","timestamp":1618441232421,"user_tz":180,"elapsed":3884,"user":{"displayName":"ARTHUR GALDINO DANGONI","photoUrl":"","userId":"07242382303046999568"}},"outputId":"540b5c8a-862b-4942-ec86-4d4c661e7089"},"source":["! pip install unidecode\n","\n","import nltk\n","\n","nltk.download('stopwords')\n","\n","stopwords_pt = nltk.corpus.stopwords.words('portuguese')\n","stopwords_en = nltk.corpus.stopwords.words('english')\n","stopwords_sp = nltk.corpus.stopwords.words('spanish')\n","stopwords = stopwords_pt + stopwords_en + stopwords_sp"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AtHMJG0jViWh"},"source":["import pandas as pd\n","import numpy as np\n","\n","def get_data(path):\n","  df_train = pd.read_csv(path + 'train.csv')\n","  df_test = pd.read_csv(path+ 'test.csv')\n","\n","  print('Loading Dataset...')\n","  print(df_train.shape)\n","  print(df_test.shape)\n","\n","  X, y = (pd.concat([df_train.Text, df_test.Text])), pd.concat([df_train.Classificacao, pd.Series(np.zeros(df_test.shape[0],))])\n","\n","  return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L75h0IIlV4W3","executionInfo":{"status":"ok","timestamp":1618441665105,"user_tz":180,"elapsed":2758,"user":{"displayName":"ARTHUR GALDINO DANGONI","photoUrl":"","userId":"07242382303046999568"}},"outputId":"9a621023-75c8-4c7a-a33c-104502174488"},"source":["import re\n","import unidecode as uc\n","\n","from collections import Counter\n","from nltk.tokenize.casual import casual_tokenize \n","from nltk import RegexpTokenizer as RT\n","\n","X,y = get_data(PATH)\n","\n","print('Cleaning text...')\n","vocabulary = []\n","bow = []\n","\n","for text in X:\n","  sentence = text\n","\n","  tokens = casual_tokenize(sentence) # strip_handles=True remove @user\n","  tokens = [x for x in tokens if x not in stopwords]\n","\n","  vocabulary += [\" \".join([x for x in tokens])]\n","# for text in X:\n","#   tokens =  RT(r'\\w+').tokenize(re.sub(r'http\\S+|@\\S+|rt\\s|#|[0-9]+|_', '', uc.unidecode(text.lower()))) \n","#   # tokens =  RT(r'\\w+').tokenize(re.sub(r'http\\S+|@\\S+|rt\\s|#', '', uc.unidecode(text.lower()))) \n","  \n","#   tokens = [token for token in tokens if token not in stopwords_pt and token not in stopwords_en and token not in stopwords_sp]\n","#   tokens = casual_tokenize(' '.join(tokens), reduce_len=True)\n","\n","#   vocabulary += [' '.join([x for x in tokens])]\n","\n","print('OK')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","if BOW:\n","  print('Creating BoW')\n","  count_vect = CountVectorizer()\n","  data = count_vect.fit_transform(vocabulary)\n","# print(\"Our vocabulary: \", count_vect.vocabulary_)\n","else:\n","  print('Creating TFidf')\n","  vectorizer = TfidfVectorizer(min_df=1)\n","  data = vectorizer.fit_transform(vocabulary)\n","\n","print(data.shape)\n","# 6053"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading Dataset...\n","(6559, 11)\n","(1640, 10)\n","Cleaning text...\n","OK\n","Creating BoW\n","(8199, 12934)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z-E0L-jZ0y-6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618441157089,"user_tz":180,"elapsed":858,"user":{"displayName":"ARTHUR GALDINO DANGONI","photoUrl":"","userId":"07242382303046999568"}},"outputId":"b23d5f9d-25a9-4291-fa14-ca5027bffd2b"},"source":["from sklearn.model_selection import train_test_split\n","\n","Xtrain, Xtest, ytrain, ytest = train_test_split(data, y, test_size=0.2, shuffle=False)\n","\n","if CASE:  \n","  Xtrain, Xtest, ytrain, ytest = train_test_split(Xtrain, ytrain, test_size=0.2, shuffle=False)\n","\n","print(Xtrain.shape, ytrain.shape)\n","print(Xtest.shape, ytest.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(6559, 12934) (6559,)\n","(1640, 12934) (1640,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"beaFMQ4yqZJR"},"source":["from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import SGDClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rgoJTHKR6TH","outputId":"9c3a2bfa-919f-4314-9cf6-c5f0bbdb62f7"},"source":["print('Grid search SGDClassifier')\n","parameters = {'loss':('hinge', 'log', 'perceptron')}\n","model = SGDClassifier(verbose=0, random_state=52, eta0=1)\n","scores = ['precision']\n","\n","\n","for score in scores:\n","  clf = GridSearchCV(model, parameters, scoring='%s_macro' % score, cv=5, verbose=3)\n","  clf.fit(Xtrain, ytrain)\n","\n","  print(\"Best parameters:\")\n","  print(clf.best_params_)\n","\n","  print(\"Grid scores: \")\n","  stds = clf.cv_results_['std_test_score']\n","  means = clf.cv_results_['mean_test_score']\n","  for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","    print(\"%0.3f (+/-%0.03f) for %r\"\n","          % (mean, std * 2, params))\n","\n","\n","# #save results\n","# ypred = clf.predict(Xtest)\n","\n","# final = pd.DataFrame(df_test.Id)\n","# final['Category'] = ypred\n","\n","# final.to_csv('/content/drive/My Drive/Datasets/ufg-nlp-2021-competition-1/teste.csv',index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Grid search SGDClassifier\n","Fitting 5 folds for each of 3 candidates, totalling 15 fits\n","[CV] loss=hinge ......................................................\n","[CV] .......................... loss=hinge, score=0.964, total=   0.1s\n","[CV] loss=hinge ......................................................\n","[CV] .......................... loss=hinge, score=0.965, total=   0.1s\n","[CV] loss=hinge ......................................................\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n","[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV] .......................... loss=hinge, score=0.965, total=   0.1s\n","[CV] loss=hinge ......................................................\n","[CV] .......................... loss=hinge, score=0.965, total=   0.1s\n","[CV] loss=hinge ......................................................\n","[CV] .......................... loss=hinge, score=0.960, total=   0.1s\n","[CV] loss=log ........................................................\n","[CV] ............................ loss=log, score=0.961, total=   0.1s\n","[CV] loss=log ........................................................\n","[CV] ............................ loss=log, score=0.968, total=   0.1s\n","[CV] loss=log ........................................................\n","[CV] ............................ loss=log, score=0.963, total=   0.1s\n","[CV] loss=log ........................................................\n","[CV] ............................ loss=log, score=0.964, total=   0.1s\n","[CV] loss=log ........................................................\n","[CV] ............................ loss=log, score=0.957, total=   0.1s\n","[CV] loss=perceptron .................................................\n","[CV] ..................... loss=perceptron, score=0.955, total=   0.1s\n","[CV] loss=perceptron .................................................\n","[CV] ..................... loss=perceptron, score=0.963, total=   0.1s\n","[CV] loss=perceptron .................................................\n","[CV] ..................... loss=perceptron, score=0.957, total=   0.1s\n","[CV] loss=perceptron .................................................\n","[CV] ..................... loss=perceptron, score=0.960, total=   0.1s\n","[CV] loss=perceptron .................................................\n","[CV] ..................... loss=perceptron, score=0.956, total=   0.1s\n","Best parameters:\n","{'loss': 'hinge'}\n","Grid scores: \n","0.964 (+/-0.004) for {'loss': 'hinge'}\n","0.962 (+/-0.007) for {'loss': 'log'}\n","0.958 (+/-0.006) for {'loss': 'perceptron'}\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.0s finished\n"],"name":"stderr"}]}]}